{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "INFO:__main__:‚úì Loaded FAISS index with 1874 vectors\n",
      "INFO:__main__:‚úì Loaded 1874 metadata entries\n",
      "INFO:__main__:‚úì Loaded 1874 text chunks\n",
      "ERROR:__main__:Error setting up Gemini: Gemini API key not provided. Set GEMINI_API_KEY environment variable or pass api_key parameter\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Gemini API key not provided. Set GEMINI_API_KEY environment variable or pass api_key parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 477\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Example usage and testing\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# Initialize RAG system\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     rag \u001b[38;5;241m=\u001b[39m \u001b[43mGermanReportsRAG\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgerman_reports_index.faiss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgerman_reports_metadata.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgerman_reports_chunks.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Will use GEMINI_API_KEY environment variable\u001b[39;49;00m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;66;03m# Example 1: General question\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Example 1: General Question ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mGermanReportsRAG.__init__\u001b[1;34m(self, index_path, metadata_path, chunks_path, model_name, gemini_api_key, gemini_model)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Load components\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_index(index_path, metadata_path, chunks_path)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG system initialized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36mGermanReportsRAG._setup_gemini\u001b[1;34m(self, api_key)\u001b[0m\n\u001b[0;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemini API key not provided. Set GEMINI_API_KEY environment variable or pass api_key parameter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Gemini API key not provided. Set GEMINI_API_KEY environment variable or pass api_key parameter"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dataclasses import dataclass\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Data class for search results\"\"\"\n",
    "    rank: int\n",
    "    score: float\n",
    "    chunk: str\n",
    "    company: str\n",
    "    year: str\n",
    "    filename: str\n",
    "    chunk_id: int\n",
    "    file_path: str\n",
    "\n",
    "class GermanReportsRAG:\n",
    "    \"\"\"RAG system for German company reports with FAISS backend and Google Gemini\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index_path: str = \"german_reports_index.faiss\",\n",
    "        metadata_path: str = \"german_reports_metadata.pkl\",\n",
    "        chunks_path: str = \"german_reports_chunks.pkl\",\n",
    "        model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        gemini_api_key: Optional[str] = None,\n",
    "        gemini_model: str = \"gemini-1.5-flash\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            index_path: Path to FAISS index file\n",
    "            metadata_path: Path to metadata pickle file\n",
    "            chunks_path: Path to chunks pickle file\n",
    "            model_name: Sentence transformer model for embeddings\n",
    "            gemini_api_key: Google Gemini API key\n",
    "            gemini_model: Gemini model to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.gemini_model = gemini_model\n",
    "        \n",
    "        # Load components\n",
    "        self._load_index(index_path, metadata_path, chunks_path)\n",
    "        self._setup_gemini(gemini_api_key)\n",
    "        \n",
    "        logger.info(f\"RAG system initialized with {len(self.chunks)} chunks\")\n",
    "    \n",
    "    def _load_index(self, index_path: str, metadata_path: str, chunks_path: str):\n",
    "        \"\"\"Load FAISS index, metadata, and chunks\"\"\"\n",
    "        try:\n",
    "            # Load FAISS index\n",
    "            self.index = faiss.read_index(index_path)\n",
    "            \n",
    "            # Load metadata\n",
    "            with open(metadata_path, 'rb') as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "            \n",
    "            # Load chunks\n",
    "            with open(chunks_path, 'rb') as f:\n",
    "                self.chunks = pickle.load(f)\n",
    "            \n",
    "            # Load sentence transformer model\n",
    "            self.encoder = SentenceTransformer(self.model_name)\n",
    "            \n",
    "            logger.info(f\"‚úì Loaded FAISS index with {self.index.ntotal} vectors\")\n",
    "            logger.info(f\"‚úì Loaded {len(self.metadata)} metadata entries\")\n",
    "            logger.info(f\"‚úì Loaded {len(self.chunks)} text chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading index components: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_gemini(self, api_key: Optional[str]):\n",
    "        \"\"\"Setup Google Gemini client\"\"\"\n",
    "        try:\n",
    "            # Get API key from parameter or environment\n",
    "            if api_key:\n",
    "                genai.configure(api_key=api_key)\n",
    "            else:\n",
    "                api_key = os.getenv('GEMINI_API_KEY')\n",
    "                if not api_key:\n",
    "                    raise ValueError(\"Gemini API key not provided. Set GEMINI_API_KEY environment variable or pass api_key parameter\")\n",
    "                genai.configure(api_key=api_key)\n",
    "            \n",
    "            # Initialize the model\n",
    "            self.gemini = genai.GenerativeModel(self.gemini_model)\n",
    "            \n",
    "            logger.info(f\"‚úì Gemini model {self.gemini_model} initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up Gemini: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_available_companies(self) -> List[str]:\n",
    "        \"\"\"Get list of all available companies in the index\"\"\"\n",
    "        companies = list(set(meta['company'] for meta in self.metadata))\n",
    "        return sorted(companies)\n",
    "    \n",
    "    def get_available_years(self, company: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Get list of all available years, optionally filtered by company\"\"\"\n",
    "        if company:\n",
    "            years = [meta['year'] for meta in self.metadata if meta['company'].lower() == company.lower()]\n",
    "        else:\n",
    "            years = [meta['year'] for meta in self.metadata]\n",
    "        return sorted(list(set(years)))\n",
    "    \n",
    "    def _filter_indices_by_metadata(\n",
    "        self, \n",
    "        company: Optional[str] = None, \n",
    "        year: Optional[Union[str, int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Filter chunk indices based on company and/or year.\n",
    "        \n",
    "        Args:\n",
    "            company: Company name to filter by\n",
    "            year: Year to filter by\n",
    "            \n",
    "        Returns:\n",
    "            List of valid chunk indices\n",
    "        \"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, meta in enumerate(self.metadata):\n",
    "            # Check company filter\n",
    "            if company and meta['company'].lower() != company.lower():\n",
    "                continue\n",
    "            \n",
    "            # Check year filter\n",
    "            if year and str(meta['year']) != str(year):\n",
    "                continue\n",
    "            \n",
    "            valid_indices.append(i)\n",
    "        \n",
    "        return valid_indices\n",
    "    \n",
    "    def search_chunks(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 10,\n",
    "        company: Optional[str] = None,\n",
    "        year: Optional[Union[str, int]] = None,\n",
    "        min_score: float = 0.0\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Search for relevant chunks with optional company/year filtering.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            company: Filter by company name\n",
    "            year: Filter by year\n",
    "            min_score: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of SearchResult objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get filtered indices\n",
    "            valid_indices = self._filter_indices_by_metadata(company, year)\n",
    "            \n",
    "            if not valid_indices:\n",
    "                logger.warning(f\"No chunks found for company={company}, year={year}\")\n",
    "                return []\n",
    "            \n",
    "            # Create query embedding\n",
    "            query_embedding = self.encoder.encode([query], convert_to_numpy=True)\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            \n",
    "            # Search in FAISS\n",
    "            scores, indices = self.index.search(query_embedding, min(k * 3, self.index.ntotal))\n",
    "            \n",
    "            # Filter results and create SearchResult objects\n",
    "            results = []\n",
    "            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "                if idx == -1 or score < min_score:\n",
    "                    continue\n",
    "                \n",
    "                # Check if this index is in our filtered set\n",
    "                if idx not in valid_indices:\n",
    "                    continue\n",
    "                \n",
    "                meta = self.metadata[idx]\n",
    "                result = SearchResult(\n",
    "                    rank=len(results) + 1,\n",
    "                    score=float(score),\n",
    "                    chunk=self.chunks[idx],\n",
    "                    company=meta['company'],\n",
    "                    year=meta['year'],\n",
    "                    filename=meta['filename'],\n",
    "                    chunk_id=meta['chunk_id'],\n",
    "                    file_path=meta['file_path']\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "                # Stop when we have enough results\n",
    "                if len(results) >= k:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} relevant chunks for query: '{query}'\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        search_results: List[SearchResult],\n",
    "        language: str = \"German\",\n",
    "        include_sources: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using Gemini based on search results.\n",
    "        \n",
    "        Args:\n",
    "            query: Original user query\n",
    "            search_results: List of relevant chunks\n",
    "            language: Language for the response\n",
    "            include_sources: Whether to include source information\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        if not search_results:\n",
    "            return f\"Keine relevanten Informationen gefunden f√ºr die Anfrage: '{query}'\"\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_parts = []\n",
    "        for result in search_results:\n",
    "            source_info = f\"[{result.company} {result.year}]\"\n",
    "            context_parts.append(f\"{source_info}: {result.chunk}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create prompt for Gemini\n",
    "        prompt = self._create_rag_prompt(query, context, language, include_sources, search_results)\n",
    "        \n",
    "        try:\n",
    "            # Generate response with Gemini\n",
    "            response = self.gemini.generate_content(prompt)\n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer with Gemini: {e}\")\n",
    "            return f\"Fehler beim Generieren der Antwort: {str(e)}\"\n",
    "    \n",
    "    def _create_rag_prompt(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: str,\n",
    "        language: str,\n",
    "        include_sources: bool,\n",
    "        search_results: List[SearchResult]\n",
    "    ) -> str:\n",
    "        \"\"\"Create the RAG prompt for Gemini\"\"\"\n",
    "        \n",
    "        sources_info = \"\"\n",
    "        if include_sources and search_results:\n",
    "            unique_sources = {}\n",
    "            for result in search_results:\n",
    "                key = f\"{result.company}_{result.year}\"\n",
    "                if key not in unique_sources:\n",
    "                    unique_sources[key] = f\"- {result.company} Jahresbericht {result.year}\"\n",
    "            sources_info = f\"\\n\\nVerf√ºgbare Quellen:\\n\" + \"\\n\".join(unique_sources.values())\n",
    "        \n",
    "        if language.lower() == \"german\":\n",
    "            prompt = f\"\"\"Sie sind ein Experte f√ºr die Analyse deutscher Unternehmensberichte. Basierend auf den bereitgestellten Informationen aus Gesch√§ftsberichten, beantworten Sie die folgende Frage pr√§zise und umfassend.\n",
    "\n",
    "FRAGE: {query}\n",
    "\n",
    "KONTEXT AUS GESCH√ÑFTSBERICHTEN:\n",
    "{context}\n",
    "\n",
    "ANWEISUNGEN:\n",
    "1. Antworten Sie ausschlie√ülich auf Deutsch\n",
    "2. Basieren Sie Ihre Antwort nur auf den bereitgestellten Informationen\n",
    "3. Wenn Sie spezifische Zahlen oder Fakten erw√§hnen, geben Sie das Unternehmen und Jahr an\n",
    "4. Falls die Informationen nicht ausreichen, sagen Sie das deutlich\n",
    "5. Strukturieren Sie Ihre Antwort klar und logisch\n",
    "6. {\"F√ºgen Sie am Ende eine Liste der verwendeten Quellen hinzu\" if include_sources else \"\"}\n",
    "\n",
    "ANTWORT:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"You are an expert in analyzing German company reports. Based on the provided information from annual reports, answer the following question precisely and comprehensively.\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "CONTEXT FROM BUSINESS REPORTS:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer in {language}\n",
    "2. Base your answer only on the provided information\n",
    "3. When mentioning specific numbers or facts, indicate the company and year\n",
    "4. If the information is insufficient, state this clearly\n",
    "5. Structure your answer clearly and logically\n",
    "6. {\"Include a list of sources used at the end\" if include_sources else \"\"}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        return prompt + sources_info\n",
    "    \n",
    "    def ask(\n",
    "        self,\n",
    "        query: str,\n",
    "        company: Optional[str] = None,\n",
    "        year: Optional[Union[str, int]] = None,\n",
    "        k: int = 5,\n",
    "        language: str = \"German\",\n",
    "        include_sources: bool = True,\n",
    "        min_score: float = 0.3\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Main RAG function: search and generate answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            company: Filter by company name\n",
    "            year: Filter by year\n",
    "            k: Number of chunks to retrieve\n",
    "            language: Response language\n",
    "            include_sources: Include source information\n",
    "            min_score: Minimum similarity threshold\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        # Search for relevant chunks\n",
    "        search_results = self.search_chunks(\n",
    "            query=query,\n",
    "            k=k,\n",
    "            company=company,\n",
    "            year=year,\n",
    "            min_score=min_score\n",
    "        )\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(\n",
    "            query=query,\n",
    "            search_results=search_results,\n",
    "            language=language,\n",
    "            include_sources=include_sources\n",
    "        )\n",
    "        \n",
    "        # Prepare response\n",
    "        response = {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'num_sources': len(search_results),\n",
    "            'sources': [\n",
    "                {\n",
    "                    'company': r.company,\n",
    "                    'year': r.year,\n",
    "                    'filename': r.filename,\n",
    "                    'score': r.score,\n",
    "                    'chunk_preview': r.chunk[:200] + \"...\" if len(r.chunk) > 200 else r.chunk\n",
    "                }\n",
    "                for r in search_results\n",
    "            ],\n",
    "            'filters_applied': {\n",
    "                'company': company,\n",
    "                'year': year\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def interactive_chat(self):\n",
    "        \"\"\"Interactive chat interface\"\"\"\n",
    "        print(\"ü§ñ German Reports RAG System\")\n",
    "        print(\"Verf√ºgbare Unternehmen:\", \", \".join(self.get_available_companies()))\n",
    "        print(\"Verf√ºgbare Jahre:\", \", \".join(self.get_available_years()))\n",
    "        print(\"\\nCommands:\")\n",
    "        print(\"- 'exit' oder 'quit' zum Beenden\")\n",
    "        print(\"- 'companies' f√ºr verf√ºgbare Unternehmen\")\n",
    "        print(\"- 'years [company]' f√ºr verf√ºgbare Jahre\")\n",
    "        print(\"- 'filter company=BMW year=2023' um Filter zu setzen\")\n",
    "        print(\"- 'clear' um Filter zu l√∂schen\")\n",
    "        \n",
    "        current_company = None\n",
    "        current_year = None\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Show current filters\n",
    "                filter_info = \"\"\n",
    "                if current_company or current_year:\n",
    "                    filter_parts = []\n",
    "                    if current_company:\n",
    "                        filter_parts.append(f\"Company: {current_company}\")\n",
    "                    if current_year:\n",
    "                        filter_parts.append(f\"Year: {current_year}\")\n",
    "                    filter_info = f\" [{', '.join(filter_parts)}]\"\n",
    "                \n",
    "                user_input = input(f\"\\nüí¨{filter_info} Ihre Frage: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"Auf Wiedersehen!\")\n",
    "                    break\n",
    "                \n",
    "                elif user_input.lower() == 'companies':\n",
    "                    print(\"Verf√ºgbare Unternehmen:\", \", \".join(self.get_available_companies()))\n",
    "                    continue\n",
    "                \n",
    "                elif user_input.lower().startswith('years'):\n",
    "                    parts = user_input.split()\n",
    "                    company = parts[1] if len(parts) > 1 else None\n",
    "                    years = self.get_available_years(company)\n",
    "                    print(f\"Verf√ºgbare Jahre{' f√ºr ' + company if company else ''}: {', '.join(years)}\")\n",
    "                    continue\n",
    "                \n",
    "                elif user_input.lower().startswith('filter'):\n",
    "                    # Parse filter command: filter company=BMW year=2023\n",
    "                    parts = user_input.split()[1:]\n",
    "                    for part in parts:\n",
    "                        if '=' in part:\n",
    "                            key, value = part.split('=', 1)\n",
    "                            if key.lower() == 'company':\n",
    "                                current_company = value\n",
    "                            elif key.lower() == 'year':\n",
    "                                current_year = value\n",
    "                    print(f\"Filter gesetzt - Company: {current_company}, Year: {current_year}\")\n",
    "                    continue\n",
    "                \n",
    "                elif user_input.lower() == 'clear':\n",
    "                    current_company = None\n",
    "                    current_year = None\n",
    "                    print(\"Filter gel√∂scht\")\n",
    "                    continue\n",
    "                \n",
    "                elif not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Process the question\n",
    "                print(\"üîç Suche nach relevanten Informationen...\")\n",
    "                response = self.ask(\n",
    "                    query=user_input,\n",
    "                    company=current_company,\n",
    "                    year=current_year,\n",
    "                    k=5,\n",
    "                    language=\"German\",\n",
    "                    include_sources=True\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nüìã Antwort ({response['num_sources']} Quellen):\")\n",
    "                print(\"=\" * 50)\n",
    "                print(response['answer'])\n",
    "                \n",
    "                if response['sources']:\n",
    "                    print(f\"\\nüìö Verwendete Quellen:\")\n",
    "                    for i, source in enumerate(response['sources'], 1):\n",
    "                        print(f\"{i}. {source['company']} {source['year']} (Score: {source['score']:.3f})\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nAuf Wiedersehen!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler: {e}\")\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag = GermanReportsRAG(\n",
    "        index_path=\"german_reports_index.faiss\",\n",
    "        metadata_path=\"german_reports_metadata.pkl\",\n",
    "        chunks_path=\"german_reports_chunks.pkl\",\n",
    "        gemini_api_key=None  # Will use GEMINI_API_KEY environment variable\n",
    "    )\n",
    "    \n",
    "    # Example 1: General question\n",
    "    print(\"=== Example 1: General Question ===\")\n",
    "    response = rag.ask(\"Wie hoch war der Umsatz?\", k=10)\n",
    "    print(f\"Answer: {response['answer']}\")\n",
    "    print(f\"Sources: {len(response['sources'])}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
